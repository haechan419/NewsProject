# main.py - AI ì±—ë´‡ FastAPI ì„œë²„ (ë§í¬ ëˆ„ë½ ìˆ˜ì •íŒ)
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict
from openai import OpenAI
import os
from dotenv import load_dotenv
import logging
import base64
from io import BytesIO
from PIL import Image
import json
from pathlib import Path
import shutil
from datetime import datetime

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê²€ìƒ‰ ì„œë¹„ìŠ¤ import
from search_service import SearchService

# ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì„œë¹„ìŠ¤ import
from trending_service import trending_service

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI Chat API",
    description="GPT-4o-mini ê¸°ë°˜ AI ì±—ë´‡ API (Tavily ê²€ìƒ‰ ì§€ì›)",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:8080",  # Spring Boot
        "http://localhost:3000",  # React (CRA)
        "http://localhost:5173",  # React (Vite)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
search_service = SearchService(client)

# ì–¼êµ´ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
FACE_DATA_DIR = Path("face_data")
FACE_DATA_DIR.mkdir(exist_ok=True)


# ===== ì•± ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸ =====
@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ì‹¤í–‰"""
    import asyncio
    logger.info("ğŸš€ FastAPI ì•± ì‹œì‘")
    # ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹  ì‹œì‘
    asyncio.create_task(trending_service.start_background_update())
    logger.info("âœ… ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹  íƒœìŠ¤í¬ ì‹œì‘ë¨")


@app.on_event("shutdown")
async def shutdown_event():
    """ì•± ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    logger.info("ğŸ›‘ FastAPI ì•± ì¢…ë£Œ")
    trending_service.stop_background_update()


# ===== Pydantic ëª¨ë¸ =====
class ConversationMessage(BaseModel):
    """ëŒ€í™” ë©”ì‹œì§€"""
    role: str
    content: str


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""
    message: str
    conversation_history: Optional[List[ConversationMessage]] = None


class SearchSource(BaseModel):
    """ê²€ìƒ‰ ì¶œì²˜"""
    title: str
    url: str
    snippet: str


class TrendingKeyword(BaseModel):
    """ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í•­ëª©"""
    rank: int
    keyword: str
    state: Optional[str] = ""  # s=ìœ ì§€, n=ì‹ ê·œ, +=ìƒìŠ¹


class TrendingData(BaseModel):
    """ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ë°ì´í„°"""
    keywords: List[TrendingKeyword]
    updated_at: Optional[str] = None
    source: str = "signal.bz"


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ"""
    reply: str
    searched: bool = False
    search_query: Optional[str] = None
    sources: Optional[List[SearchSource]] = None
    # ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê´€ë ¨ í•„ë“œ ì¶”ê°€
    is_trending: bool = False
    trending_data: Optional[TrendingData] = None


class QaRequest(BaseModel):
    """Q&A ìš”ì²­ (FAQ ì •ë³´ í¬í•¨)"""
    message: str
    faq_data: Optional[List[Dict[str, str]]] = None  # FAQ ì •ë³´
    conversation_history: Optional[List[ConversationMessage]] = None


class QaResponse(BaseModel):
    """Q&A ì‘ë‹µ"""
    reply: str


# ===== ì–¼êµ´ ì¸ì‹ ê´€ë ¨ ëª¨ë¸ (ìœ ì§€) =====
class FaceRegisterRequest(BaseModel):
    image_base64: str
    user_id: str
    user_name: Optional[str] = None


class FaceRegisterResponse(BaseModel):
    success: bool
    message: str
    face_detected: bool
    face_description: Optional[str] = None
    error: Optional[str] = None


class FaceRecognitionRequest(BaseModel):
    image_base64: str
    user_id: Optional[str] = None


class FaceRecognitionResponse(BaseModel):
    success: bool
    face_detected: bool
    face_count: int
    description: Optional[str] = None
    matched_user_id: Optional[str] = None
    matched_user_name: Optional[str] = None
    confidence: Optional[float] = None
    error: Optional[str] = None


# ===== ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ =====
def get_system_prompt(include_date: bool = True):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")
    current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    
    if include_date:
        # ë‚ ì§œ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° (ê²€ìƒ‰ ê¸°ë°˜ ì§ˆë¬¸)
        return f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**í˜„ì¬ ì‹œê°: {current_time}**
**ì˜¤ëŠ˜ ë‚ ì§œ: {current_date}**

ğŸ”´ **ì ˆëŒ€ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**
1. **ëª¨ë“  ì •ë³´ëŠ” {current_date} ê¸°ì¤€ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.**
2. **ì‹œê°„ì— ë¯¼ê°í•œ ì •ë³´(ê¸ˆìœµ ì‹œì„¸, ë‰´ìŠ¤, ë‚ ì”¨, ì£¼ê°€, ì•”í˜¸í™”í ê°€ê²© ë“±)ëŠ” ë°˜ë“œì‹œ {current_date} ê¸°ì¤€ì˜ ìµœì‹  ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.**
3. **í›ˆë ¨ ë°ì´í„°ì˜ ì˜¤ë˜ëœ ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
4. **ê¸ˆìœµ ì •ë³´(ì£¼ì‹, ì•”í˜¸í™”í, í™˜ìœ¨ ë“±)ëŠ” ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì œê³µí•˜ì„¸ìš”.**
5. **ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ "ìµœì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.**
6. **ë‹µë³€ ì‹œì‘ ì‹œ ë°˜ë“œì‹œ "{current_date} ê¸°ì¤€"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.**

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.
ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤."""
    else:
        # ë‚ ì§œ ì •ë³´ê°€ í•„ìš” ì—†ëŠ” ê²½ìš° (ì¸ì‚¬, ìê¸°ì†Œê°œ, ê°œë… ì§ˆë¬¸ ë“±)
        return """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.
ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.
ë‚ ì§œë‚˜ ì‹œê°„ ì •ë³´ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”."""

SYSTEM_PROMPT = get_system_prompt(include_date=True)
SYSTEM_PROMPT_SIMPLE = get_system_prompt(include_date=False)

FACE_ANALYSIS_PROMPT = """ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ëŒì˜ ì–¼êµ´ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
(ì–¼êµ´ ì¸ì‹ í”„ë¡¬í”„íŠ¸ ìƒëµ - ê¸°ì¡´ê³¼ ë™ì¼)
"""


# ===== ì–¼êµ´ ì¸ì‹ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) =====
def analyze_face_with_openai(image_base64: str) -> dict:
    """OpenAI Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ë¶„ì„"""
    try:
        json_prompt = FACE_ANALYSIS_PROMPT + "\n\në°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": json_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
                    ]
                }
            ],
            max_tokens=500,
            response_format={"type": "json_object"},
            temperature=0.1
        )
        
        content = response.choices[0].message.content
        if not content:
            return {"face_detected": False, "face_count": 0, "quality": "poor"}
            
        return json.loads(content)
        
    except Exception as e:
        logger.error(f"OpenAI ì–¼êµ´ ë¶„ì„ ì—ëŸ¬: {str(e)}")
        return {"face_detected": False, "face_count": 0, "quality": "poor"}


def save_face_data(user_id: str, user_name: Optional[str], image_base64: str, face_description: str):
    """ì–¼êµ´ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        user_dir = FACE_DATA_DIR / user_id
        user_dir.mkdir(exist_ok=True)
        
        image_data = base64.b64decode(image_base64)
        image = Image.open(BytesIO(image_data))
        image_path = user_dir / "face_image.png"
        image.save(image_path, "PNG")
        
        metadata = {
            "user_id": user_id,
            "user_name": user_name,
            "face_description": face_description,
            "image_path": str(image_path)
        }
        
        with open(user_dir / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logger.error(f"ë°ì´í„° ì €ì¥ ì—ëŸ¬: {e}")
        raise


def load_face_data(user_id: str) -> Optional[dict]:
    try:
        path = FACE_DATA_DIR / user_id / "metadata.json"
        if not path.exists(): return None
        with open(path, "r", encoding="utf-8") as f: return json.load(f)
    except: return None


def compare_faces(desc1: str, desc2: str) -> float:
    set1 = set(desc1.lower().split())
    set2 = set(desc2.lower().split())
    if not set2: return 0.0
    return len(set1.intersection(set2)) / max(len(set1), len(set2))


def find_matching_user(face_description: str) -> Optional[dict]:
    best_match = None
    best_conf = 0.0
    for user_dir in FACE_DATA_DIR.iterdir():
        if not user_dir.is_dir(): continue
        meta = load_face_data(user_dir.name)
        if not meta: continue
        conf = compare_faces(face_description, meta.get("face_description", ""))
        if conf > best_conf:
            best_conf = conf
            best_match = {"user_id": meta["user_id"], "user_name": meta["user_name"], "confidence": conf}
    if best_conf >= 0.3: return best_match
    return None


# ===== API ì—”ë“œí¬ì¸íŠ¸ =====
@app.get("/")
async def root():
    return {"status": "ok", "message": "AI Chat API is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "openai_configured": bool(os.getenv("OPENAI_API_KEY"))}


@app.get("/trending")
async def get_trending():
    """
    ì‹¤ì‹œê°„ ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ API
    5ë¶„ë§ˆë‹¤ ìë™ ê°±ì‹ ë˜ëŠ” ìºì‹œëœ ë°ì´í„° ë°˜í™˜
    """
    logger.info("ğŸ”¥ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì¡°íšŒ ìš”ì²­")
    cache = trending_service.get_cached_keywords()
    
    if not cache.get("keywords"):
        # ìºì‹œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¦‰ì‹œ ê°±ì‹  ì‹œë„
        logger.info("ìºì‹œê°€ ë¹„ì–´ìˆì–´ ì¦‰ì‹œ ê°±ì‹  ì‹œë„...")
        await trending_service.update_cache()
        cache = trending_service.get_cached_keywords()
    
    if not cache.get("keywords"):
        raise HTTPException(
            status_code=503,
            detail="í˜„ì¬ ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
    
    return {
        "keywords": cache.get("keywords", []),
        "updated_at": cache.get("updated_at"),
        "source": cache.get("source", "signal.bz")
    }


# [ìˆ˜ì •ë¨] ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ - ë§í¬ ëˆ„ë½ ë¬¸ì œ í•´ê²° + ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì§€ì›
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    AI ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    - ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì§ˆë¬¸ ê°ì§€ ì‹œ trending_service ì‚¬ìš©
    - ì¼ë°˜ ì§ˆë¬¸ì€ ê¸°ì¡´ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì‚¬ìš©
    """
    logger.info(f"ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ : {request.message[:50]}...")
    
    try:
        # Step 0: ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ë¨¼ì € í™•ì¸
        if trending_service.is_trending_question(request.message):
            logger.info("ğŸ”¥ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì§ˆë¬¸ ê°ì§€ë¨ - trending_service ì‚¬ìš©")
            trending_result = trending_service.format_trending_response()
            
            # TrendingData ë³€í™˜
            trending_data = None
            if trending_result.get("trending_data"):
                td = trending_result["trending_data"]
                trending_data = TrendingData(
                    keywords=[TrendingKeyword(**kw) for kw in td.get("keywords", [])],
                    updated_at=td.get("updated_at"),
                    source=td.get("source", "signal.bz")
                )
            
            return ChatResponse(
                reply=trending_result["reply"],
                searched=False,
                is_trending=True,
                trending_data=trending_data
            )
        
        # Step 1: ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¡œ ë©”ì‹œì§€ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        search_result = search_service.process_message(
            message=request.message,
            conversation_history=[
                {"role": msg.role, "content": msg.content}
                for msg in (request.conversation_history or [])
            ]
        )
        
        # ê²€ìƒ‰ ê²°ê³¼(sources) ì•ˆì „í•˜ê²Œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def get_safe_sources(result_dict):
            sources = []
            if result_dict.get("sources"):
                for src in result_dict["sources"]:
                    try:
                        # í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸ í›„ ì¶”ê°€
                        if src.get("title") and src.get("url"):
                            sources.append(SearchSource(**src))
                    except Exception as e:
                        logger.warning(f"ì†ŒìŠ¤ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•¨): {e}")
                        continue
            return sources

        # Step 2: ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ë‹µë³€(Reply)ê¹Œì§€ ì™„ì„±í•´ì„œ ì¤€ ê²½ìš°
        if search_result.get("reply"):
            logger.info(f"ê²€ìƒ‰ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            return ChatResponse(
                reply=search_result["reply"],
                searched=True,
                search_query=search_result.get("search_query"),
                sources=get_safe_sources(search_result)
            )
        
        # Step 3: ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ë‹µë³€ì„ ëª» ì¤¬ê±°ë‚˜(None), ê²€ìƒ‰ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
        # ì¼ë°˜ GPT ì‘ë‹µ ìƒì„±
        current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        
        # ê²€ìƒ‰ì´ ìŠ¤í‚µë˜ì—ˆëŠ”ì§€ í™•ì¸ (ê²€ìƒ‰ì´ í•„ìš” ì—†ëŠ” ì§ˆë¬¸ì¸ ê²½ìš°)
        is_search_skipped = search_result.get("searched") == False and not search_result.get("reply")
        
        # ê²€ìƒ‰ì´ ìŠ¤í‚µëœ ê²½ìš°ëŠ” ë‚ ì§œ ì •ë³´ ì—†ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        system_prompt = SYSTEM_PROMPT_SIMPLE if is_search_skipped else SYSTEM_PROMPT
        messages = [{"role": "system", "content": system_prompt}]
        
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({"role": msg.role, "content": msg.content})
        
        # ê²€ìƒ‰ì´ ìŠ¤í‚µëœ ê²½ìš°(ì¸ì‚¬, ìê¸°ì†Œê°œ, ê°œë… ì§ˆë¬¸ ë“±)ëŠ” ë‚ ì§œ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
        if is_search_skipped:
            # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ë‚ ì§œ ì •ë³´ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
            messages.append({"role": "user", "content": request.message})
        else:
            # ê²€ìƒ‰ì„ ì‹œë„í–ˆì§€ë§Œ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë‚ ì§œ ì •ë³´ í¬í•¨
            user_message_with_time = f"""âš ï¸ ì¤‘ìš”: í˜„ì¬ ë‚ ì§œëŠ” {current_date}ì…ë‹ˆë‹¤. ëª¨ë“  ì •ë³´ëŠ” ì´ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

{request.message}

**ë°˜ë“œì‹œ {current_date} ê¸°ì¤€ì˜ ìµœì‹  ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ , í›ˆë ¨ ë°ì´í„°ì˜ ì˜¤ë˜ëœ ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**"""
            messages.append({"role": "user", "content": user_message_with_time})
        
        # ê¸ˆìœµ ì •ë³´ì¸ì§€ í™•ì¸í•˜ì—¬ temperature ì¡°ì •
        financial_keywords = ['ê°€ê²©', 'ì‹œì„¸', 'ì‹œì¥ê°€', 'í˜„ì¬ê°€', 'ë¹„íŠ¸ì½”ì¸', 'BTC', 'ì´ë”ë¦¬ì›€', 'ETH', 
                             'ì£¼ì‹', 'ì½”ì¸', 'ì•”í˜¸í™”í', 'ê°€ìƒí™”í', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ì‹œê°€ì´ì•¡', 'ê±°ë˜ëŸ‰',
                             'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'LG', 'í˜„ëŒ€ì°¨', 'ê¸°ì•„', 'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤']
        is_financial = any(keyword in request.message for keyword in financial_keywords)
        temperature = 0.3 if is_financial else 0.5
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=1000,
            temperature=temperature,
        )
        reply = response.choices[0].message.content
        logger.info(f"ì¼ë°˜ GPT ì‘ë‹µ ìƒì„± ì™„ë£Œ: {reply[:50]}...")
        
        # [ì¤‘ìš” ìˆ˜ì •] ê²€ìƒ‰ì€ ì‹œë„í–ˆê³ (searched=True), ê²°ê³¼(sources)ê°€ ìˆë‹¤ë©´
        # ì¼ë°˜ ì‘ë‹µì´ë¼ë„ ë§í¬ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ë°˜í™˜!
        if search_result.get("searched"):
            logger.info("ì¼ë°˜ ì‘ë‹µì— ê²€ìƒ‰ ì¶œì²˜ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.")
            return ChatResponse(
                reply=reply,
                searched=True,
                search_query=search_result.get("search_query"),
                sources=get_safe_sources(search_result)  # ê¸°ì¡´ì—ëŠ” ì—¬ê¸°ì„œ []ë¥¼ ë°˜í™˜í•´ì„œ ë¬¸ì œì˜€ìŒ
            )
        
        return ChatResponse(reply=reply, searched=False)
        
    except Exception as e:
        logger.error(f"ì±„íŒ… ì—ëŸ¬: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ===== ì–¼êµ´ ì¸ì‹ API ì—”ë“œí¬ì¸íŠ¸ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) =====
@app.post("/face/register", response_model=FaceRegisterResponse)
async def register_face(request: FaceRegisterRequest):
    # (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼ - ìƒëµ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”)
    logger.info(f"ì–¼êµ´ ë“±ë¡ ìš”ì²­: {request.user_id}")
    try:
        if "," in request.image_base64: img_b64 = request.image_base64.split(",")[1]
        else: img_b64 = request.image_base64
        
        img_data = base64.b64decode(img_b64)
        img = Image.open(BytesIO(img_data))
        buf = BytesIO()
        img.save(buf, format="PNG")
        img_png = base64.b64encode(buf.getvalue()).decode()
        
        res = analyze_face_with_openai(img_png)
        if not res.get("face_detected"):
            return FaceRegisterResponse(success=False, message="ì–¼êµ´ ë¯¸ê°ì§€", face_detected=False, error="No face")
            
        save_face_data(request.user_id, request.user_name, img_png, res.get("face_description", ""))
        return FaceRegisterResponse(success=True, message="ë“±ë¡ ì™„ë£Œ", face_detected=True)
    except Exception as e:
        logger.error(f"ë“±ë¡ ì—ëŸ¬: {e}")
        return FaceRegisterResponse(success=False, message=str(e), face_detected=False, error=str(e))

@app.post("/face/recognize", response_model=FaceRecognitionResponse)
async def recognize_face(request: FaceRecognitionRequest):
    # (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    try:
        if "," in request.image_base64: img_b64 = request.image_base64.split(",")[1]
        else: img_b64 = request.image_base64
        
        img_data = base64.b64decode(img_b64)
        img = Image.open(BytesIO(img_data))
        buf = BytesIO()
        img.save(buf, format="PNG")
        img_png = base64.b64encode(buf.getvalue()).decode()
        
        res = analyze_face_with_openai(img_png)
        if not res.get("face_detected"):
            return FaceRecognitionResponse(success=True, face_detected=False, face_count=0)
            
        match = find_matching_user(res.get("face_description", ""))
        return FaceRecognitionResponse(
            success=True, face_detected=True, face_count=res.get("face_count", 1),
            matched_user_id=match["user_id"] if match else None,
            matched_user_name=match["user_name"] if match else None,
            confidence=match["confidence"] if match else None
        )
    except Exception as e:
        logger.error(f"ì¸ì‹ ì—ëŸ¬: {e}")
        return FaceRecognitionResponse(success=False, face_detected=False, face_count=0, error=str(e))

@app.delete("/face/{user_id}")
async def delete_face(user_id: str):
    try:
        path = FACE_DATA_DIR / user_id
        if path.exists(): shutil.rmtree(path)
        return {"success": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/face/{user_id}")
async def get_face_info(user_id: str):
    meta = load_face_data(user_id)
    if not meta: raise HTTPException(status_code=404)
    return {"user_id": meta["user_id"], "user_name": meta["user_name"], "registered": True}


# ===== Q&A ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (FAQ ê¸°ë°˜) =====
@app.post("/qa", response_model=QaResponse)
async def qa_chat(request: QaRequest):
    """
    Q&A ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (FAQ ê¸°ë°˜)
    FAQ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ ë°›ì•„ì„œ GPT-4o-minië¡œ ë‹µë³€ ìƒì„±
    """
    logger.info(f"Q&A ìš”ì²­ ìˆ˜ì‹ : {request.message[:50]}...")
    logger.info(f"FAQ ë°ì´í„° ìˆ˜ì‹ : {len(request.faq_data) if request.faq_data else 0}ê±´")
    
    try:
        # ì¸ì‚¬ë§ ë° ìê¸°ì†Œê°œ ì§ˆë¬¸ ê°ì§€ (FAQ ì—†ì´ ì²˜ë¦¬)
        greeting_keywords = ['ì•ˆë…•', 'ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•í•˜ì…¨ìŠµë‹ˆê¹Œ', 'í•˜ì´', 'hi', 'hello', 'ë°˜ê°€ì›Œ', 'ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ì¢‹ì€ ì•„ì¹¨', 'ì¢‹ì€ ì €ë…', 'ì•ˆë…•íˆê°€ì„¸ìš”', 'ì•ˆë…•íˆê³„ì„¸ìš”']
        self_intro_keywords = ['ë‹¹ì‹ ì€', 'ë„ˆëŠ”', 'ë„ˆëŠ” ëˆ„êµ¬', 'ë‹¹ì‹ ì€ ëˆ„êµ¬', 'ëˆ„êµ¬ì„¸ìš”', 'ëˆ„êµ¬ì•¼', 'ì†Œê°œ', 'ìê¸°ì†Œê°œ', 'ë­í•˜ëŠ”', 'ë¬´ì—‡ì„', 'ì—­í• ', 'ê¸°ëŠ¥', 'ë­ì•¼', 'ë­í•˜ëŠ”ê±°ì•¼', 'ë­í•˜ëŠ” ê±°ì•¼', 'ë­í•˜ëŠ”ê±°', 'ë­í•˜ëŠ” ê±°']
        user_message_lower = request.message.lower().strip()
        is_greeting = any(keyword in user_message_lower for keyword in greeting_keywords) or len(user_message_lower) <= 5
        is_self_intro = any(keyword in user_message_lower for keyword in self_intro_keywords)
        
        # ì¸ì‚¬ë§ì´ë‚˜ ìê¸°ì†Œê°œ ì§ˆë¬¸ì¸ ê²½ìš° FAQ ì—†ì´ë„ ë‹µë³€
        if is_greeting or is_self_intro:
            logger.info(f"{'ì¸ì‚¬ë§' if is_greeting else 'ìê¸°ì†Œê°œ ì§ˆë¬¸'}ë¡œ íŒë‹¨í•˜ì—¬ FAQ ì—†ì´ ì²˜ë¦¬")
            if is_greeting:
                system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ í”Œë«í¼ ê³ ê°ì„¼í„°ì˜ ì¹œì ˆí•œ AI ìƒë‹´ì›ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì¸ì‚¬ì— ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•˜ê³ , ì–´ë–¤ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆëŠ”ì§€ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
            else:
                system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ í”Œë«í¼ ê³ ê°ì„¼í„°ì˜ ì¹œì ˆí•œ AI ìƒë‹´ì›ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì—ê²Œ ìì‹ ì„ ì†Œê°œí•˜ê³ , ì–´ë–¤ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ë‰´ìŠ¤ í”Œë«í¼ì˜ ê³ ê°ì„¼í„° AI ìƒë‹´ì›ì„ì„ ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”
- ì„œë¹„ìŠ¤ ì´ìš© ê´€ë ¨ ì§ˆë¬¸ì— ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”
- ì˜ìƒ ì œì‘, ê²Œì‹œë¬¼ ì‘ì„±, í”„ë¡œí•„/ê³„ì • ê´€ë¦¬ ë“± ì„œë¹„ìŠ¤ ì´ìš© ê´€ë ¨ ì§ˆë¬¸ì„ ë°›ì„ ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•˜ì„¸ìš”
í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
            
            messages = [{"role": "system", "content": system_prompt}]
            if request.conversation_history:
                for msg in request.conversation_history:
                    messages.append({"role": msg.role, "content": msg.content})
            messages.append({"role": "user", "content": request.message})
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=500,
                temperature=0.7,
            )
            reply = response.choices[0].message.content
            logger.info(f"{'ì¸ì‚¬ë§' if is_greeting else 'ìê¸°ì†Œê°œ'} ì‘ë‹µ ìƒì„± ì™„ë£Œ: {reply[:50]}...")
            return QaResponse(reply=reply)
        
        # FAQ ë°ì´í„° ë¡œê¹…
        has_faq_data = request.faq_data and len(request.faq_data) > 0
        if has_faq_data:
            for i, faq in enumerate(request.faq_data, 1):
                logger.info(f"FAQ {i}: ì¹´í…Œê³ ë¦¬={faq.get('category', 'N/A')}, ì§ˆë¬¸={faq.get('question', 'N/A')[:50]}...")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (FAQ ì •ë³´ í¬í•¨)
        system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ í”Œë«í¼ ê³ ê°ì„¼í„°ì˜ ì¹œì ˆí•œ AI ìƒë‹´ì›ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ê´€ë ¨ëœ ê²ƒì¸ì§€ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”.
   - ì„œë¹„ìŠ¤ ì´ìš© ê´€ë ¨: ì˜ìƒ ì œì‘, ê²Œì‹œë¬¼ ì‘ì„±, í”„ë¡œí•„/ê³„ì •, ë¡œê·¸ì¸, íšŒì›ê°€ì…, ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°, ë¬¸ì˜í•˜ê¸°, ê³„ì • ì„¤ì •, í”„ë¡œí•„ ìˆ˜ì • ë“±
   - ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ë¬´ê´€: ì¼ë°˜ì ì¸ ê°œë… ì§ˆë¬¸(ì•¼ë‹¹ì´ ë­ì•¼, ì‚°ë¶ˆ ì–´ë””ì„œ ë‚¬ì–´, ë¹„íŠ¸ì½”ì¸ì´ ë­ì•¼ ë“±), ë‰´ìŠ¤ ì§ˆë¬¸, ì •ì¹˜/ê²½ì œ ì¼ë°˜ ìƒì‹ ë“±
2. ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë©´ FAQ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
3. ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ë¬´ê´€í•œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ FAQ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì´ìš© ê´€ë ¨ ì§ˆë¬¸ë§Œ ë‹µë³€í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ 'ë¬¸ì˜ í‹°ì¼“ ì‘ì„±'ì„ í†µí•´ ë¬¸ì˜í•´ì£¼ì‹œë©´ ê´€ë¦¬ìê°€ í™•ì¸ í›„ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ë©°, ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
5. ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"""
        
        if has_faq_data:
            system_prompt += "ë‹¤ìŒì€ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” FAQ ì •ë³´ì…ë‹ˆë‹¤:\n\n"
            for i, faq in enumerate(request.faq_data, 1):
                system_prompt += f"--- FAQ {i} ---\n"
                system_prompt += f"ì¹´í…Œê³ ë¦¬: {faq.get('category', '')}\n"
                system_prompt += f"ì§ˆë¬¸: {faq.get('question', '')}\n"
                system_prompt += f"ë‹µë³€: {faq.get('answer', '')}\n\n"
            system_prompt += """ìœ„ FAQ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
FAQì— ê´€ë ¨ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•´ë‹¹ FAQì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""
        else:
            system_prompt += """FAQ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ê´€ë ¨ëœ ê²ƒì¸ì§€ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”.
ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë©´ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ê³ ,
ì„œë¹„ìŠ¤ ì´ìš©ê³¼ ë¬´ê´€í•œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë©´ ìœ„ ê·œì¹™ 3ë²ˆì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”."""
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({"role": msg.role, "content": msg.content})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        messages.append({"role": "user", "content": request.message})
        
        # GPT-4o-mini í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=1000,
            temperature=0.3,  # FAQ ê¸°ë°˜ ë‹µë³€ì´ë¯€ë¡œ ë‚®ì€ temperature ì‚¬ìš©
        )
        
        reply = response.choices[0].message.content
        logger.info(f"Q&A ì‘ë‹µ ìƒì„± ì™„ë£Œ: {reply[:50]}...")
        
        return QaResponse(reply=reply)
        
    except Exception as e:
        logger.error(f"Q&A ì—ëŸ¬: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True, log_level="info")